{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shobhiii/super-resolution-using-pretrained-mae?scriptVersionId=230384264\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"b0db01ac","metadata":{"papermill":{"duration":0.006794,"end_time":"2025-03-29T15:51:28.242115","exception":false,"start_time":"2025-03-29T15:51:28.235321","status":"completed"},"tags":[]},"source":["### Imports Required Libraries  \n","This code imports essential libraries for deep learning (PyTorch, torchvision), image processing (skimage), and utility tools (tqdm, matplotlib) for training and evaluating models."]},{"cell_type":"code","execution_count":1,"id":"e09a0e2f","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:28.255588Z","iopub.status.busy":"2025-03-29T15:51:28.255272Z","iopub.status.idle":"2025-03-29T15:51:36.59576Z","shell.execute_reply":"2025-03-29T15:51:36.595057Z"},"papermill":{"duration":8.349213,"end_time":"2025-03-29T15:51:36.597432","exception":false,"start_time":"2025-03-29T15:51:28.248219","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from einops import rearrange\n","from skimage.metrics import structural_similarity as ssim\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torchvision.models import vgg16\n","from skimage.metrics import structural_similarity as ssim\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","import time\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"3de36bbc","metadata":{"papermill":{"duration":0.005725,"end_time":"2025-03-29T15:51:36.609627","exception":false,"start_time":"2025-03-29T15:51:36.603902","status":"completed"},"tags":[]},"source":["### Custom Dataset and DataLoader Creation  \n","Defines `ImageDataset` to load low-resolution and high-resolution image pairs from `.npy` files.  \n","The `create_dataloaders` function splits the dataset into training and validation sets and returns corresponding DataLoaders.\n"]},{"cell_type":"code","execution_count":2,"id":"e3d62001","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.622422Z","iopub.status.busy":"2025-03-29T15:51:36.622038Z","iopub.status.idle":"2025-03-29T15:51:36.629625Z","shell.execute_reply":"2025-03-29T15:51:36.62891Z"},"papermill":{"duration":0.01554,"end_time":"2025-03-29T15:51:36.631014","exception":false,"start_time":"2025-03-29T15:51:36.615474","status":"completed"},"tags":[]},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self, low_res_path, high_res_path):\n","        self.low_res_path = low_res_path\n","        self.high_res_path = high_res_path\n","        self.file_names = sorted(os.listdir(low_res_path))  \n","    \n","    def __len__(self):\n","        return len(self.file_names)\n","    \n","    def __getitem__(self, idx):\n","        file_name = self.file_names[idx]\n","        low_res = np.load(os.path.join(self.low_res_path, file_name))\n","        high_res = np.load(os.path.join(self.high_res_path, file_name))\n","        low_res = np.repeat(low_res, 3, axis=0)  # (3, 75, 75)\n","        high_res = np.repeat(high_res, 3, axis=0)\n","        \n","        return torch.tensor(low_res, dtype=torch.float32), torch.tensor(high_res, dtype=torch.float32)\n","\n","def create_dataloaders(base_path, batch_size=32):\n","    \"\"\"\n","    Returns:\n","        tuple: (train_loader, val_loader)\n","    \"\"\"\n","    low_res_path = os.path.join(base_path, \"LR\")\n","    high_res_path = os.path.join(base_path, \"HR\")\n","    dataset = ImageDataset(low_res_path, high_res_path)\n","    \n","    val_size = int(0.1 * len(dataset))\n","    train_size = len(dataset) - val_size\n","    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","    \n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    \n","    return train_loader, val_loader"]},{"cell_type":"markdown","id":"bd3280bf","metadata":{"papermill":{"duration":0.005639,"end_time":"2025-03-29T15:51:36.642957","exception":false,"start_time":"2025-03-29T15:51:36.637318","status":"completed"},"tags":[]},"source":["### Patch Embedding Layer  \n","Defines `PatchEmbed`, a module that converts an image into a sequence of patch embeddings using a convolutional layer.  \n","It splits the input image into non-overlapping patches and projects them into an embedding space.\n"]},{"cell_type":"code","execution_count":3,"id":"53a41822","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.656603Z","iopub.status.busy":"2025-03-29T15:51:36.656334Z","iopub.status.idle":"2025-03-29T15:51:36.66108Z","shell.execute_reply":"2025-03-29T15:51:36.660406Z"},"papermill":{"duration":0.013333,"end_time":"2025-03-29T15:51:36.662328","exception":false,"start_time":"2025-03-29T15:51:36.648995","status":"completed"},"tags":[]},"outputs":[],"source":["class PatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding \"\"\"\n","    def __init__(self, img_size=75, patch_size=16, in_chans=3, embed_dim=1024):\n","        super().__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size//patch_size)**2\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        B, C, H, W = x.shape\n","        x = self.proj(x)\n","        return x"]},{"cell_type":"markdown","id":"6b8fc75b","metadata":{"papermill":{"duration":0.005601,"end_time":"2025-03-29T15:51:36.67395","exception":false,"start_time":"2025-03-29T15:51:36.668349","status":"completed"},"tags":[]},"source":["### Multi-Head Self-Attention Module  \n","Defines an `Attention` module that implements multi-head self-attention.  \n","It computes query, key, and value projections, applies scaled dot-product attention, and processes the output through linear layers with dropout.\n"]},{"cell_type":"code","execution_count":4,"id":"5207bdf2","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.686596Z","iopub.status.busy":"2025-03-29T15:51:36.686311Z","iopub.status.idle":"2025-03-29T15:51:36.692462Z","shell.execute_reply":"2025-03-29T15:51:36.691676Z"},"papermill":{"duration":0.01416,"end_time":"2025-03-29T15:51:36.693848","exception":false,"start_time":"2025-03-29T15:51:36.679688","status":"completed"},"tags":[]},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=16, qkv_bias=True, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n"]},{"cell_type":"markdown","id":"e0ede9f1","metadata":{"papermill":{"duration":0.005714,"end_time":"2025-03-29T15:51:36.705579","exception":false,"start_time":"2025-03-29T15:51:36.699865","status":"completed"},"tags":[]},"source":["### Multi-Layer Perceptron (MLP) Block  \n","Defines an `Mlp` module with two fully connected layers, an activation function (GELU), and dropout.  \n","It expands the input features and applies non-linearity, commonly used in transformer architectures.\n"]},{"cell_type":"code","execution_count":5,"id":"9c0e4d44","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.718125Z","iopub.status.busy":"2025-03-29T15:51:36.717865Z","iopub.status.idle":"2025-03-29T15:51:36.722798Z","shell.execute_reply":"2025-03-29T15:51:36.722073Z"},"papermill":{"duration":0.012669,"end_time":"2025-03-29T15:51:36.724031","exception":false,"start_time":"2025-03-29T15:51:36.711362","status":"completed"},"tags":[]},"outputs":[],"source":["class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features * 4\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x"]},{"cell_type":"markdown","id":"b9305682","metadata":{"papermill":{"duration":0.005613,"end_time":"2025-03-29T15:51:36.735722","exception":false,"start_time":"2025-03-29T15:51:36.730109","status":"completed"},"tags":[]},"source":["### Transformer Block  \n","Defines a `TransformerBlock` consisting of multi-head self-attention, layer normalization, and a feed-forward MLP.  \n","It follows the standard Transformer architecture with residual connections and normalization layers.\n"]},{"cell_type":"code","execution_count":6,"id":"2bef46ba","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.748204Z","iopub.status.busy":"2025-03-29T15:51:36.747941Z","iopub.status.idle":"2025-03-29T15:51:36.753387Z","shell.execute_reply":"2025-03-29T15:51:36.752562Z"},"papermill":{"duration":0.013308,"end_time":"2025-03-29T15:51:36.754804","exception":false,"start_time":"2025-03-29T15:51:36.741496","status":"completed"},"tags":[]},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path = 0.,\n","                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.norm1(x))\n","        x = x + self.mlp(self.norm2(x))\n","        return x"]},{"cell_type":"markdown","id":"d0b1791d","metadata":{"papermill":{"duration":0.005748,"end_time":"2025-03-29T15:51:36.766411","exception":false,"start_time":"2025-03-29T15:51:36.760663","status":"completed"},"tags":[]},"source":["### Transformer Encoder  \n","Defines a `TransformerEncoder` that processes images by embedding patches and applying multiple Transformer blocks.  \n","It includes positional embeddings, normalization, and a stack of self-attention layers for feature extraction.\n"]},{"cell_type":"code","execution_count":7,"id":"9e712c98","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.77933Z","iopub.status.busy":"2025-03-29T15:51:36.779072Z","iopub.status.idle":"2025-03-29T15:51:36.784869Z","shell.execute_reply":"2025-03-29T15:51:36.7842Z"},"papermill":{"duration":0.013412,"end_time":"2025-03-29T15:51:36.786019","exception":false,"start_time":"2025-03-29T15:51:36.772607","status":"completed"},"tags":[]},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, img_size=75, patch_size=16, in_chans=3, embed_dim=1024, depth=24, \n","                 num_heads=16, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.):\n","        super().__init__()\n","        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n","        num_patches = (img_size // patch_size) ** 2\n","        \n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","        self.blocks = nn.ModuleList([\n","            TransformerBlock(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n","                drop=drop_rate, attn_drop=attn_drop_rate)\n","            for i in range(depth)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        # Extract patches\n","        x = self.patch_embed(x)\n","        x = rearrange(x, 'b c h w -> b (h w) c')  # Flatten spatial dims into sequence\n","        \n","        # Add positional embedding\n","        x = x + self.pos_embed\n","        \n","        # Apply transformer blocks\n","        for blk in self.blocks:\n","            x = blk(x)\n","        \n","        x = self.norm(x)\n","        return x"]},{"cell_type":"markdown","id":"aa85fe38","metadata":{"papermill":{"duration":0.005693,"end_time":"2025-03-29T15:51:36.79757","exception":false,"start_time":"2025-03-29T15:51:36.791877","status":"completed"},"tags":[]},"source":["### Transformer Decoder  \n","Defines a `TransformerDecoder` that reconstructs the full image from encoded patches and mask tokens.  \n","It applies a series of Transformer blocks and normalization to refine the decoded representation.\n"]},{"cell_type":"code","execution_count":8,"id":"a8b752b8","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.81019Z","iopub.status.busy":"2025-03-29T15:51:36.809928Z","iopub.status.idle":"2025-03-29T15:51:36.815075Z","shell.execute_reply":"2025-03-29T15:51:36.81428Z"},"papermill":{"duration":0.012795,"end_time":"2025-03-29T15:51:36.816216","exception":false,"start_time":"2025-03-29T15:51:36.803421","status":"completed"},"tags":[]},"outputs":[],"source":["class TransformerDecoder(nn.Module):\n","    \"\"\"\n","    Decoder will be used to reconstruct the full image from the encoded visible patches and mask tokens.\n","    \"\"\"\n","    def __init__(self,\n","                embed_dim = 768,\n","                depth = 8,\n","                num_heads = 16,\n","                mlp_ratio = 4.,\n","                qkv_bias = True,\n","                drop_rate = 0.,\n","                attn_drop_rate = 0.,\n","                drop_path_rate = 0.):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([\n","            TransformerBlock(\n","                dim = embed_dim,\n","                num_heads = num_heads,\n","                mlp_ratio = mlp_ratio,\n","                qkv_bias = qkv_bias,\n","                drop = drop_rate,\n","                attn_drop = attn_drop_rate,\n","                drop_path = drop_path_rate * i / depth\n","            ) for i in range(depth)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","        \n","    def forward(self, x):\n","        for block in self.blocks:\n","            x  = block(x)\n","        x = self.norm(x)\n","        return x"]},{"cell_type":"markdown","id":"95e03384","metadata":{"papermill":{"duration":0.005582,"end_time":"2025-03-29T15:51:36.827807","exception":false,"start_time":"2025-03-29T15:51:36.822225","status":"completed"},"tags":[]},"source":["### Super-Resolution Transformer Decoder  \n","Defines `SRTransformerDecoder`, a Transformer-based decoder for super-resolution tasks.  \n","It processes encoded features using multiple Transformer blocks and applies layer normalization to refine the output.\n"]},{"cell_type":"code","execution_count":9,"id":"36f745be","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.840113Z","iopub.status.busy":"2025-03-29T15:51:36.839854Z","iopub.status.idle":"2025-03-29T15:51:36.844504Z","shell.execute_reply":"2025-03-29T15:51:36.843867Z"},"papermill":{"duration":0.012258,"end_time":"2025-03-29T15:51:36.845711","exception":false,"start_time":"2025-03-29T15:51:36.833453","status":"completed"},"tags":[]},"outputs":[],"source":["class SRTransformerDecoder(nn.Module):\n","    def __init__(self, embed_dim=512, depth=8, num_heads=8, mlp_ratio=4., \n","                 qkv_bias=True, drop_rate=0., attn_drop_rate=0.):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([\n","            TransformerBlock(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n","                drop=drop_rate, attn_drop=attn_drop_rate)\n","            for i in range(depth)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","        return x\n"]},{"cell_type":"markdown","id":"91c7bb74","metadata":{"papermill":{"duration":0.005627,"end_time":"2025-03-29T15:51:36.857057","exception":false,"start_time":"2025-03-29T15:51:36.85143","status":"completed"},"tags":[]},"source":["### Pixel Shuffle Upsampling  \n","Defines `PixelShuffleUpsample`, which upsamples feature maps using a convolutional layer followed by pixel shuffle.  \n","It increases the spatial resolution while maintaining feature integrity and applies GELU activation.\n"]},{"cell_type":"code","execution_count":10,"id":"80dcd295","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.86957Z","iopub.status.busy":"2025-03-29T15:51:36.869314Z","iopub.status.idle":"2025-03-29T15:51:36.873645Z","shell.execute_reply":"2025-03-29T15:51:36.872944Z"},"papermill":{"duration":0.012016,"end_time":"2025-03-29T15:51:36.874913","exception":false,"start_time":"2025-03-29T15:51:36.862897","status":"completed"},"tags":[]},"outputs":[],"source":["class PixelShuffleUpsample(nn.Module):\n","    def __init__(self, in_features, scale_factor=2):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_features, in_features * scale_factor**2, kernel_size=3, padding=1)\n","        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n","        self.act = nn.GELU()\n","        \n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.pixel_shuffle(x)\n","        x = self.act(x)\n","        return x"]},{"cell_type":"markdown","id":"f52e2781","metadata":{"papermill":{"duration":0.005685,"end_time":"2025-03-29T15:51:36.886481","exception":false,"start_time":"2025-03-29T15:51:36.880796","status":"completed"},"tags":[]},"source":["## Masked Autoencoder (MAE) for Image Reconstruction\n","\n","This model implements a **Masked Autoencoder (MAE)** using a Transformer-based architecture.  \n","It randomly masks image patches, encodes visible ones, and reconstructs the full image using a decoder.  \n","The MAE learns image representations by predicting the missing patches, optimizing reconstruction loss.  \n"]},{"cell_type":"code","execution_count":11,"id":"7dddcff2","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.899184Z","iopub.status.busy":"2025-03-29T15:51:36.898923Z","iopub.status.idle":"2025-03-29T15:51:36.915259Z","shell.execute_reply":"2025-03-29T15:51:36.914542Z"},"papermill":{"duration":0.024274,"end_time":"2025-03-29T15:51:36.916479","exception":false,"start_time":"2025-03-29T15:51:36.892205","status":"completed"},"tags":[]},"outputs":[],"source":["class MaskedAutoEncoder(nn.Module):\n","    def __init__(self,\n","                img_size = 224,\n","                patch_size = 16,\n","                in_chans = 3,\n","                embed_dim = 1024,\n","                depth = 24,\n","                num_heads = 16,\n","                decoder_embed_dim = 512,\n","                decoder_depth = 8,\n","                decoder_num_heads = 16,\n","                mlp_ratio = 4.,\n","                norm_layer = nn.LayerNorm):\n","        super().__init__()\n","        # Encoder Components\n","        # PatchEmbed splits the image into patches and embeds them\n","        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n","        self.num_patches = self.patch_embed.num_patches\n","\n","        # Class token and Positional Encoding for encoder\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n","\n","        # Encoder for the visible Patches\n","        self.encoder = TransformerEncoder(\n","            embed_dim = embed_dim,\n","            depth = depth,\n","            num_heads = num_heads,\n","            mlp_ratio = mlp_ratio\n","        )\n","\n","        # Decoder components\n","        # Convert the encoder output to decoder dimension\n","        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias = True)\n","        # Learnable mask token that is used for masked patches\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","        # Positional encoding ffor the decoder\n","        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim))\n","        \n","        # Decoder to reconstruct the full image\n","        self.decoder = TransformerDecoder(\n","            embed_dim = decoder_embed_dim,\n","            depth = decoder_depth,\n","            num_heads = decoder_num_heads,\n","            mlp_ratio = mlp_ratio\n","            )\n","        # Final prediction Layer: predict pixel values for eeach patch\n","        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias = True)\n","        \n","        # Initialize weights for all components\n","        self.initialize_weights()\n","\n","        # Store model parameters for later use\n","        self.patch_size = patch_size\n","        self.in_chans = in_chans\n","        self.img_size = img_size\n","\n","    def initialize_weights(self):\n","        pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.pos_embed.shape[2]))\n","        decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.decoder_pos_embed.shape[2]))\n","        \n","        \n","        # Use truncated normal distributions \n","        nn.init.trunc_normal_(self.pos_embed, std = 0.02)\n","        nn.init.trunc_normal_(self.decoder_pos_embed, std = 0.02)\n","\n","        nn.init.trunc_normal_(self.cls_token, std = 0.02)\n","        nn.init.trunc_normal_(self.mask_token, std = 0.02)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.trunc_normal_(m.weight, std = 0.02)\n","            if m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def random_masking(self, x, mask_ratio):\n","        N, L, D = x.shape # Batch, length, dimension\n","        len_keep = int(L * (1 - mask_ratio)) # Number of patches to keep\n","\n","        # Generate uniform random noise for each patch in each sample\n","        noise = torch.rand(N, L, device = x.device) \n","\n","        # Sort noise to determine which patches to keep/remove\n","        ids_shuffle = torch.argsort(noise, dim=1)\n","        ids_restore = torch.argsort(ids_shuffle, dim=1)\n","\n","        # Keep the first len_keep patches (lowest noise values)\n","        ids_keep = ids_shuffle[:, :len_keep]\n","        x_masked = torch.gather(x, dim=1, index = ids_keep.unsqueeze(-1).repeat(1, 1, D))\n","\n","        mask = torch.ones([N, L], device = x.device)\n","        mask[:, :len_keep] = 0\n","        # Unshuffle to get the binary mask for original sequence\n","        mask = torch.gather(mask, dim = 1, index = ids_restore)\n","\n","        return x_masked, mask, ids_restore\n","\n","    def forward_encoder(self, x, mask_ratio):\n","        # Convert img to patches\n","        x = self.patch_embed(x)\n","    \n","        # Add positional embeddings\n","        cls_token = self.cls_token + self.pos_embed[:, :1, :]  # [1, 1, D]\n","        x = x + self.pos_embed[:, 1:, :]  # [B, N, D]\n","    \n","        # Apply random masking\n","        x, mask, ids_restore = self.random_masking(x, mask_ratio)  # [B, N', D]\n","\n","        # Expand class token to match batch size\n","        cls_tokens = cls_token.expand(x.shape[0], -1, -1)  # [B, 1, D]\n","\n","        # Concatenate cls_token and image tokens\n","        x = torch.cat((cls_tokens, x), dim=1)  # [B, N'+1, D]\n","\n","        # Process through transformer encoder\n","        x = self.encoder(x)\n","\n","        return x, mask, ids_restore\n","\n","\n","    def forward_decoder(self, x, ids_restore):\n","        # embed the encoder output\n","        x = self.decoder_embed(x)\n","\n","        # add mask tokens\n","        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n","\n","        # exclude class token x[:, 1:] and append mask tokens\n","        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim = 1)\n","\n","        # unshuffle: restore the original sequence order\n","        x_ = torch.gather(x_, dim = 1, index = ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))\n","\n","        # append class token\n","        x = torch.cat([x[:, :1, :], x_], dim = 1)\n","\n","        # apply positional embedding \n","        x = x + self.decoder_pos_embed\n","\n","        # apply transformer decoder \n","        x = self.decoder(x)\n","\n","        # predict pixel values for each patch\n","        x = self.decoder_pred(x)\n","\n","        # remove class token from prediictions\n","        x = x[:, 1:, :]\n","\n","        return x\n","\n","    def forward(self, imgs, mask_ratio = 0.75):\n","        # Forward Pass the entire MAE model\n","\n","        # run encoder on the images with masking\n","        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n","\n","        # run decoder to predict all patches\n","        pred = self.forward_decoder(latent, ids_restore)\n","\n","        # convert input images to patches for loss calculation\n","        target = self.patchify(imgs)\n","\n","        # calculate mse loss only for masked patches\n","        loss = self.calculate_loss(pred, target, mask)\n","\n","        return loss, pred, mask\n","\n","    def patchify(self, imgs):\n","        # convert imgs to patches for calculating loss\n","        p = self.patch_size\n","        h = w = self.img_size//p\n","\n","        x = rearrange(imgs, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n","        return x\n","\n","    def unpatchify(self, x):\n","        p = self.patch_size\n","        h = w = int(x.shape[1] ** 0.5)\n","\n","        imgs = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h = h, w = w, p1 = p, p2 = p)\n","        return imgs\n","\n","    def calculate_loss(self, pred, target, mask):\n","        # calculate mse loss for masked patches only\n","\n","        loss = (pred - target)**2\n","        loss = loss.mean(dim = -1)\n","\n","        loss = (loss*mask).sum()/mask.sum() \n","\n","        return loss"]},{"cell_type":"markdown","id":"9a82dfb3","metadata":{"papermill":{"duration":0.005883,"end_time":"2025-03-29T15:51:36.928659","exception":false,"start_time":"2025-03-29T15:51:36.922776","status":"completed"},"tags":[]},"source":["## Super-Resolution Model using Transformer-based MAE\n","\n","This model builds upon a **Masked Autoencoder (MAE)** to perform **image super-resolution**.  \n","It encodes low-resolution images, processes them with a Transformer-based decoder, and progressively upsamples them to a higher resolution.  \n","The model also supports loading pretrained MAE weights for better initialization and improved performance.  \n"]},{"cell_type":"code","execution_count":12,"id":"42de10cd","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.941559Z","iopub.status.busy":"2025-03-29T15:51:36.941247Z","iopub.status.idle":"2025-03-29T15:51:36.955219Z","shell.execute_reply":"2025-03-29T15:51:36.954533Z"},"papermill":{"duration":0.022021,"end_time":"2025-03-29T15:51:36.956478","exception":false,"start_time":"2025-03-29T15:51:36.934457","status":"completed"},"tags":[]},"outputs":[],"source":["class SuperResolutionModel(nn.Module):\n","    def __init__(self, img_size=75, patch_size=15, in_chans=3, encoder_embed_dim=1024, \n","                 decoder_embed_dim=512, encoder_depth=24, decoder_depth=8, encoder_num_heads=16, \n","                 decoder_num_heads=8, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.):\n","        super().__init__()\n","        \n","        # Encoder from MAE (pretrained)\n","        self.encoder = TransformerEncoder(\n","            img_size=img_size, \n","            patch_size=patch_size, \n","            in_chans=in_chans,\n","            embed_dim=encoder_embed_dim, \n","            depth=encoder_depth,\n","            num_heads=encoder_num_heads, \n","            mlp_ratio=mlp_ratio,\n","            qkv_bias=qkv_bias, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate\n","        )\n","        \n","        # Bridge from encoder to SR decoder\n","        self.sr_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim)\n","        \n","        # SR-specific decoder\n","        self.sr_decoder = SRTransformerDecoder(\n","            embed_dim=decoder_embed_dim,\n","            depth=decoder_depth,\n","            num_heads=decoder_num_heads,\n","            mlp_ratio=mlp_ratio,\n","            qkv_bias=qkv_bias,\n","            drop_rate=drop_rate,\n","            attn_drop_rate=attn_drop_rate\n","        )\n","        \n","        # Calculate output sequence length (patches)\n","        self.num_patches = (img_size // patch_size) ** 2\n","        self.patch_size = patch_size\n","        \n","        # Adjusted spatial dimensions after reshaping\n","        self.patches_h = self.patches_w = img_size // patch_size\n","        \n","        # Progressive upsampling layers\n","        self.upsampling = nn.Sequential(\n","            nn.Conv2d(decoder_embed_dim, decoder_embed_dim * 4, kernel_size=3, padding=1),\n","            nn.PixelShuffle(upscale_factor=2),  # 5x5 → 10x10\n","            nn.GELU(),\n","    \n","            nn.Conv2d(decoder_embed_dim, decoder_embed_dim * 4, kernel_size=3, padding=1),\n","            nn.PixelShuffle(upscale_factor=2),  # 10x10 → 20x20\n","            nn.GELU(),\n","    \n","            nn.Conv2d(decoder_embed_dim, decoder_embed_dim * 4, kernel_size=3, padding=1),\n","            nn.PixelShuffle(upscale_factor=2),  # 20x20 → 40x40\n","            nn.GELU(),\n","\n","            nn.Conv2d(decoder_embed_dim, decoder_embed_dim * 4, kernel_size=3, padding=1),\n","            nn.PixelShuffle(upscale_factor=2),  # 40x40 → 80x80\n","            nn.GELU(),\n","\n","            nn.Upsample(size=(150, 150), mode=\"bilinear\", align_corners=True)  # Final resize\n","        )\n","        \n","        # Final projection to RGB\n","        self.final_conv = nn.Conv2d(decoder_embed_dim, in_chans, kernel_size=3, padding=1)\n","        \n","    def forward(self, x):\n","        # Encode\n","        features = self.encoder(x)\n","        \n","        # Bridge to decoder embedding dimension\n","        features = self.sr_embed(features)\n","        \n","        # Decode\n","        features = self.sr_decoder(features)\n","        \n","        # Reshape to spatial format\n","        features = rearrange(features, 'b (h w) c -> b c h w', h=self.patches_h, w=self.patches_w)\n","        \n","        # Upsample\n","        features = self.upsampling(features)\n","        \n","        # Final projection to output image\n","        output = self.final_conv(features)\n","        \n","        return output\n","\n","\n","    def load_from_mae(self, mae_model):\n","        # Load encoder\n","        self.encoder.patch_embed.proj.weight.data = mae_model.patch_embed.proj.weight.data\n","        self.encoder.patch_embed.proj.bias.data = mae_model.patch_embed.proj.bias.data\n","        \n","        # Adjust position embeddings for potentially different input size\n","        if self.encoder.pos_embed.shape != mae_model.encoder.pos_embed.shape:\n","            # Interpolate position embeddings to new size\n","            pos_embed = mae_model.encoder.pos_embed\n","            src_size = int(mae_model.encoder.pos_embed.shape[1] ** 0.5)\n","            tgt_size = int(self.encoder.pos_embed.shape[1] ** 0.5)\n","            \n","            pos_embed = pos_embed.reshape(1, src_size, src_size, -1).permute(0, 3, 1, 2)\n","            pos_embed = F.interpolate(pos_embed, size=(tgt_size, tgt_size), mode='bicubic')\n","            pos_embed = pos_embed.permute(0, 2, 3, 1).reshape(1, tgt_size*tgt_size, -1)\n","            self.encoder.pos_embed.data = pos_embed\n","        else:\n","            self.encoder.pos_embed.data = mae_model.encoder.pos_embed.data\n","            \n","        # Load transformer blocks\n","        for i, blk in enumerate(self.encoder.blocks):\n","            blk.load_state_dict(mae_model.encoder.blocks[i].state_dict())\n","            \n","        # Load encoder norm\n","        self.encoder.norm.load_state_dict(mae_model.encoder.norm.state_dict())\n","        \n","        # Bridge embed (initialize from MAE decoder_embed)\n","        self.sr_embed.weight.data = mae_model.decoder_embed.weight.data\n","        self.sr_embed.bias.data = mae_model.decoder_embed.bias.data\n","        \n","        # Initialize decoder from MAE decoder (if dimensions match)\n","        for i, blk in enumerate(self.sr_decoder.blocks):\n","            if i < len(mae_model.decoder.blocks):\n","                # Only load parameters with matching dimensions\n","                mae_blk = mae_model.decoder.blocks[i]\n","                \n","                # Check and load attention\n","                if blk.attn.qkv.weight.shape == mae_blk.attn.qkv.weight.shape:\n","                    blk.attn.qkv.load_state_dict(mae_blk.attn.qkv.state_dict())\n","                if blk.attn.proj.weight.shape == mae_blk.attn.proj.weight.shape:\n","                    blk.attn.proj.load_state_dict(mae_blk.attn.proj.state_dict())\n","                \n","                # Check and load MLP\n","                if blk.mlp.fc1.weight.shape == mae_blk.mlp.fc1.weight.shape:\n","                    blk.mlp.fc1.load_state_dict(mae_blk.mlp.fc1.state_dict())\n","                if blk.mlp.fc2.weight.shape == mae_blk.mlp.fc2.weight.shape:\n","                    blk.mlp.fc2.load_state_dict(mae_blk.mlp.fc2.state_dict())\n","                \n","                # Load norms (if dimensions match)\n","                if blk.norm1.weight.shape == mae_blk.norm1.weight.shape:\n","                    blk.norm1.load_state_dict(mae_blk.norm1.state_dict())\n","                if blk.norm2.weight.shape == mae_blk.norm2.weight.shape:\n","                    blk.norm2.load_state_dict(mae_blk.norm2.state_dict())\n","        \n","        # Load decoder norm\n","        if self.sr_decoder.norm.weight.shape == mae_model.decoder.norm.weight.shape:\n","            self.sr_decoder.norm.load_state_dict(mae_model.decoder.norm.state_dict())"]},{"cell_type":"markdown","id":"249a18c5","metadata":{"papermill":{"duration":0.005623,"end_time":"2025-03-29T15:51:36.968227","exception":false,"start_time":"2025-03-29T15:51:36.962604","status":"completed"},"tags":[]},"source":["### VGG Perceptual Loss\n","This class implements a perceptual loss function using a pre-trained VGG16 network. It extracts hierarchical features from input images and computes an L1 loss at multiple layers to measure perceptual similarity. The loss is commonly used for tasks like image super-resolution and style transfer.\n"]},{"cell_type":"code","execution_count":13,"id":"eb00246e","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:36.980792Z","iopub.status.busy":"2025-03-29T15:51:36.980496Z","iopub.status.idle":"2025-03-29T15:51:36.989028Z","shell.execute_reply":"2025-03-29T15:51:36.988385Z"},"papermill":{"duration":0.016285,"end_time":"2025-03-29T15:51:36.99032","exception":false,"start_time":"2025-03-29T15:51:36.974035","status":"completed"},"tags":[]},"outputs":[],"source":["class VGGPerceptualLoss(nn.Module):\n","    def __init__(self, resize=True):\n","        super(VGGPerceptualLoss, self).__init__()\n","        vgg_pretrained = vgg16(pretrained=True).features\n","        self.slice1 = torch.nn.Sequential()\n","        self.slice2 = torch.nn.Sequential()\n","        self.slice3 = torch.nn.Sequential()\n","        self.slice4 = torch.nn.Sequential()\n","        \n","        for x in range(4):\n","            self.slice1.add_module(str(x), vgg_pretrained[x])\n","        for x in range(4, 9):\n","            self.slice2.add_module(str(x), vgg_pretrained[x])\n","        for x in range(9, 16):\n","            self.slice3.add_module(str(x), vgg_pretrained[x])\n","        for x in range(16, 23):\n","            self.slice4.add_module(str(x), vgg_pretrained[x])\n","            \n","        for param in self.parameters():\n","            param.requires_grad = False\n","        \n","        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4]\n","        self.resize = resize\n","        \n","    def forward(self, x, y):\n","        if self.resize:\n","            x = nn.functional.interpolate(x, mode='bilinear', size=(224, 224), align_corners=False)\n","            y = nn.functional.interpolate(y, mode='bilinear', size=(224, 224), align_corners=False)\n","        \n","        loss = 0.0\n","        x_vgg, y_vgg = self.preprocess(x), self.preprocess(y)\n","        \n","        x_feat1 = self.slice1(x_vgg)\n","        y_feat1 = self.slice1(y_vgg)\n","        loss += self.weights[0] * nn.functional.l1_loss(x_feat1, y_feat1)\n","        \n","        x_feat2 = self.slice2(x_feat1)\n","        y_feat2 = self.slice2(y_feat1)\n","        loss += self.weights[1] * nn.functional.l1_loss(x_feat2, y_feat2)\n","        \n","        x_feat3 = self.slice3(x_feat2)\n","        y_feat3 = self.slice3(y_feat2)\n","        loss += self.weights[2] * nn.functional.l1_loss(x_feat3, y_feat3)\n","        \n","        x_feat4 = self.slice4(x_feat3)\n","        y_feat4 = self.slice4(y_feat3)\n","        loss += self.weights[3] * nn.functional.l1_loss(x_feat4, y_feat4)\n","        \n","        return loss\n","    \n","    def preprocess(self, x):\n","        # Normalize to match VGG input\n","        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n","        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n","        return (x - mean) / std"]},{"cell_type":"markdown","id":"a99fbdc6","metadata":{"papermill":{"duration":0.005691,"end_time":"2025-03-29T15:51:37.001989","exception":false,"start_time":"2025-03-29T15:51:36.996298","status":"completed"},"tags":[]},"source":["### Image Quality Metrics Calculation  \n","This function computes MSE, SSIM, and PSNR for a batch of images by comparing predicted and target images. It processes images in the [0,1] range and returns the average metrics, commonly used for image restoration and super-resolution evaluation.\n"]},{"cell_type":"code","execution_count":14,"id":"4162be06","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:37.014727Z","iopub.status.busy":"2025-03-29T15:51:37.014406Z","iopub.status.idle":"2025-03-29T15:51:37.020055Z","shell.execute_reply":"2025-03-29T15:51:37.019205Z"},"papermill":{"duration":0.013542,"end_time":"2025-03-29T15:51:37.021278","exception":false,"start_time":"2025-03-29T15:51:37.007736","status":"completed"},"tags":[]},"outputs":[],"source":["def calculate_metrics(pred, target):\n","    # Convert tensors to numpy arrays\n","    pred = pred.detach().cpu().numpy().transpose(0, 2, 3, 1)  # B, H, W, C\n","    target = target.detach().cpu().numpy().transpose(0, 2, 3, 1)  # B, H, W, C\n","    \n","    # Initialize metrics\n","    batch_mse = 0\n","    batch_ssim = 0\n","    batch_psnr = 0\n","    batch_size = pred.shape[0]\n","    \n","    # Calculate metrics for each image in batch\n","    for i in range(batch_size):\n","        # Clip values to valid image range [0, 1]\n","        p = np.clip(pred[i], 0, 1)\n","        t = np.clip(target[i], 0, 1)\n","        \n","        # MSE\n","        mse = np.mean((p - t) ** 2)\n","        batch_mse += mse\n","        \n","        # SSIM (multichannel for RGB)\n","        data_range = t.max() - t.min()\n","        batch_ssim += ssim(p, t, multichannel=True, channel_axis=2, data_range = data_range)\n","        \n","        # PSNR\n","        batch_psnr += psnr(t, p, data_range=1.0)\n","    \n","    # Return average metrics\n","    return {\n","        'mse': batch_mse / batch_size,\n","        'ssim': batch_ssim / batch_size,\n","        'psnr': batch_psnr / batch_size\n","    }\n"]},{"cell_type":"code","execution_count":15,"id":"5a81e133","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:37.034076Z","iopub.status.busy":"2025-03-29T15:51:37.033835Z","iopub.status.idle":"2025-03-29T15:51:37.040229Z","shell.execute_reply":"2025-03-29T15:51:37.039549Z"},"papermill":{"duration":0.014114,"end_time":"2025-03-29T15:51:37.041447","exception":false,"start_time":"2025-03-29T15:51:37.027333","status":"completed"},"tags":[]},"outputs":[],"source":["def plot_training_curves(history, save_dir):\n","    \"\"\"Plot and save training curves\"\"\"\n","    # Create figure with subplots\n","    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n","    \n","    # Plot loss curves\n","    axs[0, 0].plot(history['train_loss'], label='Train Loss')\n","    axs[0, 0].plot(history['val_loss'], label='Validation Loss')\n","    axs[0, 0].set_title('Loss')\n","    axs[0, 0].set_xlabel('Epoch')\n","    axs[0, 0].set_ylabel('Loss')\n","    axs[0, 0].legend()\n","    \n","    # Plot MSE\n","    axs[0, 1].plot(history['val_mse'], label='Validation MSE')\n","    axs[0, 1].set_title('Mean Squared Error (MSE)')\n","    axs[0, 1].set_xlabel('Epoch')\n","    axs[0, 1].set_ylabel('MSE')\n","    axs[0, 1].legend()\n","    \n","    # Plot SSIM\n","    axs[1, 0].plot(history['val_ssim'], label='Validation SSIM')\n","    axs[1, 0].set_title('Structural Similarity Index (SSIM)')\n","    axs[1, 0].set_xlabel('Epoch')\n","    axs[1, 0].set_ylabel('SSIM')\n","    axs[1, 0].legend()\n","    \n","    # Plot PSNR\n","    axs[1, 1].plot(history['val_psnr'], label='Validation PSNR')\n","    axs[1, 1].set_title('Peak Signal-to-Noise Ratio (PSNR)')\n","    axs[1, 1].set_xlabel('Epoch')\n","    axs[1, 1].set_ylabel('PSNR (dB)')\n","    axs[1, 1].legend()\n","    \n","    plt.tight_layout()\n","    \n","    # Save plot\n","    plt.savefig(os.path.join(save_dir, 'training_curves.png'))\n","    plt.close()"]},{"cell_type":"markdown","id":"ff48e474","metadata":{"papermill":{"duration":0.005835,"end_time":"2025-03-29T15:51:37.053282","exception":false,"start_time":"2025-03-29T15:51:37.047447","status":"completed"},"tags":[]},"source":["### Super-Resolution Model Training  \n","This function trains a super-resolution model using both pixel-wise (L1) and perceptual (VGG-based) loss. It leverages an MAE-pretrained model, tracks validation metrics (MSE, SSIM, PSNR), and saves the best model based on PSNR. Training progress is logged, and model checkpoints are saved periodically.\n"]},{"cell_type":"code","execution_count":16,"id":"09be85df","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:37.066022Z","iopub.status.busy":"2025-03-29T15:51:37.065747Z","iopub.status.idle":"2025-03-29T15:51:37.077179Z","shell.execute_reply":"2025-03-29T15:51:37.076474Z"},"papermill":{"duration":0.019192,"end_time":"2025-03-29T15:51:37.078391","exception":false,"start_time":"2025-03-29T15:51:37.059199","status":"completed"},"tags":[]},"outputs":[],"source":["def train_model(model, mae_model, train_dataloader, val_dataloader, device, \n","               num_epochs=50, save_dir='./checkpoints'):\n","    \n","    # Create save directory if it doesn't exist\n","    os.makedirs(save_dir, exist_ok=True)\n","    \n","    # Load weights from MAE model\n","    model.load_from_mae(mae_model)\n","    model = model.to(device)\n","    \n","    # Initialize losses\n","    criterion_pixel = nn.L1Loss()\n","    criterion_perceptual = VGGPerceptualLoss().to(device)\n","    \n","    # Initialize optimizer and scheduler\n","    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n","    \n","    # Training statistics\n","    history = {\n","        'train_loss': [],\n","        'val_loss': [],\n","        'val_mse': [],\n","        'val_ssim': [],\n","        'val_psnr': []\n","    }\n","    \n","    best_psnr = 0\n","    \n","    # Start training\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        \n","        # Training phase\n","        model.train()\n","        train_loss = 0\n","        \n","        progress_bar = tqdm(train_dataloader, desc=f\"Training\")\n","        for batch_idx, (low_res, high_res) in enumerate(progress_bar):\n","            low_res, high_res = low_res.to(device), high_res.to(device)\n","            \n","            # Forward pass\n","            optimizer.zero_grad()\n","            # low_res = low_res.repeat(1, 3, 1, 1)\n","            outputs = model(low_res)\n","            \n","            # Calculate losses\n","            pixel_loss = criterion_pixel(outputs, high_res)\n","            perceptual_loss = criterion_perceptual(outputs, high_res)\n","            loss = pixel_loss + 0.1 * perceptual_loss\n","            \n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","            \n","            # Update statistics\n","            train_loss += loss.item()\n","            progress_bar.set_postfix({\"loss\": loss.item()})\n","        \n","        # Calculate average training loss\n","        train_loss /= len(train_dataloader)\n","        history['train_loss'].append(train_loss)\n","        \n","        # Validation phase\n","        model.eval()\n","        val_loss = 0\n","        val_metrics = {'mse': 0, 'ssim': 0, 'psnr': 0}\n","        \n","        with torch.no_grad():\n","            progress_bar = tqdm(val_dataloader, desc=f\"Validation\")\n","            for batch_idx, (low_res, high_res) in enumerate(progress_bar):\n","                low_res, high_res = low_res.to(device), high_res.to(device)\n","                \n","                # Forward pass\n","                outputs = model(low_res)\n","                \n","                # Calculate losses\n","                pixel_loss = criterion_pixel(outputs, high_res)\n","                perceptual_loss = criterion_perceptual(outputs, high_res)\n","                loss = pixel_loss + 0.1 * perceptual_loss\n","                \n","                # Update statistics\n","                val_loss += loss.item()\n","                \n","                # Calculate metrics\n","                metrics = calculate_metrics(outputs, high_res)\n","                val_metrics['mse'] += metrics['mse']\n","                val_metrics['ssim'] += metrics['ssim']\n","                val_metrics['psnr'] += metrics['psnr']\n","                \n","                progress_bar.set_postfix({\"val_loss\": loss.item()})\n","        \n","        # Calculate average validation loss and metrics\n","        val_loss /= len(val_dataloader)\n","        val_metrics['mse'] /= len(val_dataloader)\n","        val_metrics['ssim'] /= len(val_dataloader)\n","        val_metrics['psnr'] /= len(val_dataloader)\n","        \n","        # Update history\n","        history['val_loss'].append(val_loss)\n","        history['val_mse'].append(val_metrics['mse'])\n","        history['val_ssim'].append(val_metrics['ssim'])\n","        history['val_psnr'].append(val_metrics['psnr'])\n","        \n","        # Update scheduler\n","        scheduler.step()\n","        \n","        # Print epoch statistics\n","        print(f\"Train Loss: {train_loss:.6f}\")\n","        print(f\"Val Loss: {val_loss:.6f}, MSE: {val_metrics['mse']:.6f}, \"\n","              f\"SSIM: {val_metrics['ssim']:.6f}, PSNR: {val_metrics['psnr']:.6f}\")\n","        \n","        # Save best model based on PSNR\n","        if val_metrics['psnr'] > best_psnr:\n","            best_psnr = val_metrics['psnr']\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': val_loss,\n","                'psnr': val_metrics['psnr'],\n","                'ssim': val_metrics['ssim'],\n","                'mse': val_metrics['mse'],\n","            }, os.path.join(save_dir, 'best_model.pth'))\n","            print(f\"Saved best model with PSNR: {best_psnr:.6f}\")\n","        \n","        # Save checkpoint every 5 epochs\n","        if (epoch + 1) % 5 == 0:\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': val_loss,\n","                'history': history,\n","            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n","    \n","    # Plot training curves\n","    plot_training_curves(history, save_dir)\n","    \n","    return model, history"]},{"cell_type":"code","execution_count":17,"id":"5207b1f5","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:37.09134Z","iopub.status.busy":"2025-03-29T15:51:37.091076Z","iopub.status.idle":"2025-03-29T15:51:37.181956Z","shell.execute_reply":"2025-03-29T15:51:37.181034Z"},"papermill":{"duration":0.099097,"end_time":"2025-03-29T15:51:37.18351","exception":false,"start_time":"2025-03-29T15:51:37.084413","status":"completed"},"tags":[]},"outputs":[],"source":["base_path = \"/kaggle/input/dataset-6-for-sr/Dataset\"\n","train_dataloader, val_dataloader = create_dataloaders(base_path, batch_size=8)"]},{"cell_type":"code","execution_count":18,"id":"6e122ddc","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:37.196524Z","iopub.status.busy":"2025-03-29T15:51:37.196254Z","iopub.status.idle":"2025-03-29T15:51:37.25473Z","shell.execute_reply":"2025-03-29T15:51:37.25406Z"},"papermill":{"duration":0.0664,"end_time":"2025-03-29T15:51:37.25611","exception":false,"start_time":"2025-03-29T15:51:37.18971","status":"completed"},"tags":[]},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)"]},{"cell_type":"code","execution_count":19,"id":"4a6b93df","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:37.268886Z","iopub.status.busy":"2025-03-29T15:51:37.268663Z","iopub.status.idle":"2025-03-29T15:51:53.152705Z","shell.execute_reply":"2025-03-29T15:51:53.151905Z"},"papermill":{"duration":15.891748,"end_time":"2025-03-29T15:51:53.154108","exception":false,"start_time":"2025-03-29T15:51:37.26236","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-19-b7d99f9c845b>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  mae_checkpoint = torch.load('/kaggle/input/pretrained-masked-auto-encoder/pytorch/default/1/model.pth')\n"]},{"data":{"text/plain":["_IncompatibleKeys(missing_keys=['encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias'], unexpected_keys=[])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["mae_model = MaskedAutoEncoder().to(device)\n","mae_checkpoint = torch.load('/kaggle/input/pretrained-masked-auto-encoder/pytorch/default/1/model.pth')\n","mae_model.load_state_dict(mae_checkpoint, strict = False)"]},{"cell_type":"code","execution_count":20,"id":"2f44e00f","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:53.167763Z","iopub.status.busy":"2025-03-29T15:51:53.167446Z","iopub.status.idle":"2025-03-29T15:51:55.964764Z","shell.execute_reply":"2025-03-29T15:51:55.963899Z"},"papermill":{"duration":2.80559,"end_time":"2025-03-29T15:51:55.966311","exception":false,"start_time":"2025-03-29T15:51:53.160721","status":"completed"},"tags":[]},"outputs":[],"source":["sr_model = SuperResolutionModel(\n","    img_size=75,  # Low-res input size\n","    patch_size=16,\n","    in_chans=3,\n","    encoder_embed_dim=1024,  \n","    decoder_embed_dim=512,   \n","    encoder_depth=24,        \n","    decoder_depth=8          \n",")"]},{"cell_type":"code","execution_count":21,"id":"542a3678","metadata":{"execution":{"iopub.execute_input":"2025-03-29T15:51:55.979809Z","iopub.status.busy":"2025-03-29T15:51:55.979513Z","iopub.status.idle":"2025-03-29T16:53:30.027573Z","shell.execute_reply":"2025-03-29T16:53:30.026884Z"},"papermill":{"duration":3694.056451,"end_time":"2025-03-29T16:53:30.029181","exception":false,"start_time":"2025-03-29T15:51:55.97273","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:02<00:00, 224MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [07:57<00:00,  2.36it/s, loss=0.0125]\n","Validation: 100%|██████████| 125/125 [00:35<00:00,  3.53it/s, val_loss=0.0129]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.016772\n","Val Loss: 0.012808, MSE: 0.000186, SSIM: 0.960664, PSNR: 37.466745\n","Saved best model with PSNR: 37.466745\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:26<00:00,  3.44it/s, loss=0.0118]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.64it/s, val_loss=0.0121]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.012518\n","Val Loss: 0.012079, MSE: 0.000144, SSIM: 0.962628, PSNR: 38.535957\n","Saved best model with PSNR: 38.535957\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:23<00:00,  3.48it/s, loss=0.0114]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.70it/s, val_loss=0.0118]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.012064\n","Val Loss: 0.011818, MSE: 0.000131, SSIM: 0.963415, PSNR: 38.933978\n","Saved best model with PSNR: 38.933978\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:23<00:00,  3.48it/s, loss=0.0117]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.65it/s, val_loss=0.0118]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.011764\n","Val Loss: 0.011736, MSE: 0.000127, SSIM: 0.963533, PSNR: 39.038091\n","Saved best model with PSNR: 39.038091\n","Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:23<00:00,  3.47it/s, loss=0.0117]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.60it/s, val_loss=0.0115]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.011568\n","Val Loss: 0.011472, MSE: 0.000111, SSIM: 0.964075, PSNR: 39.613521\n","Saved best model with PSNR: 39.613521\n","Epoch 6/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:23<00:00,  3.48it/s, loss=0.0113]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.62it/s, val_loss=0.0115]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.011420\n","Val Loss: 0.011509, MSE: 0.000114, SSIM: 0.964036, PSNR: 39.479521\n","Epoch 7/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:22<00:00,  3.49it/s, loss=0.0116]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.58it/s, val_loss=0.0113]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.011271\n","Val Loss: 0.011291, MSE: 0.000103, SSIM: 0.964433, PSNR: 39.935734\n","Saved best model with PSNR: 39.935734\n","Epoch 8/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:23<00:00,  3.48it/s, loss=0.0113]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.66it/s, val_loss=0.0112]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.011191\n","Val Loss: 0.011228, MSE: 0.000100, SSIM: 0.964523, PSNR: 40.039625\n","Saved best model with PSNR: 40.039625\n","Epoch 9/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:22<00:00,  3.48it/s, loss=0.0112]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.82it/s, val_loss=0.0112]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.011130\n","Val Loss: 0.011165, MSE: 0.000097, SSIM: 0.964601, PSNR: 40.160305\n","Saved best model with PSNR: 40.160305\n","Epoch 10/10\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1125/1125 [05:23<00:00,  3.48it/s, loss=0.0112]\n","Validation: 100%|██████████| 125/125 [00:18<00:00,  6.66it/s, val_loss=0.0112]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.011095\n","Val Loss: 0.011139, MSE: 0.000096, SSIM: 0.964683, PSNR: 40.209333\n","Saved best model with PSNR: 40.209333\n"]}],"source":["trained_model, history = train_model(\n","    model=sr_model,\n","    mae_model=mae_model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    device=device,\n","    num_epochs=10,\n","    save_dir='./sr_checkpoints'\n",")"]},{"cell_type":"code","execution_count":null,"id":"b864d5b2","metadata":{"execution":{"iopub.status.busy":"2025-03-29T01:10:28.140528Z","iopub.status.idle":"2025-03-29T01:10:28.14079Z","shell.execute_reply":"2025-03-29T01:10:28.140676Z"},"papermill":{"duration":1.324946,"end_time":"2025-03-29T16:53:32.683184","exception":false,"start_time":"2025-03-29T16:53:31.358238","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"323e8c81","metadata":{"papermill":{"duration":1.303195,"end_time":"2025-03-29T16:53:35.289983","exception":false,"start_time":"2025-03-29T16:53:33.986788","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6980726,"sourceId":11183355,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":278094,"modelInstanceId":256772,"sourceId":300600,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":3747.708892,"end_time":"2025-03-29T16:53:53.204584","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-29T15:51:25.495692","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}