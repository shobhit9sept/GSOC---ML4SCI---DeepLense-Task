{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11130478,"sourceType":"datasetVersion","datasetId":6941817},{"sourceId":300600,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":256772,"modelId":278094}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Required Libraries  \nThis section imports essential libraries for image classification using Vision Transformers (ViT), including PyTorch, torchvision, Transformers, and sklearn for model training, preprocessing, and evaluation.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport os\nfrom torchvision import datasets\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor\nfrom torchvision.transforms import functional as F\nfrom torchvision import transforms\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, precision_score, recall_score, f1_score\nfrom itertools import cycle\nfrom einops import rearrange, repeat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:39.706217Z","iopub.execute_input":"2025-03-27T05:26:39.706521Z","iopub.status.idle":"2025-03-27T05:26:39.711678Z","shell.execute_reply.started":"2025-03-27T05:26:39.706500Z","shell.execute_reply":"2025-03-27T05:26:39.710894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Custom Dataset and DataLoader Creation  \nDefines `NpyDataset`, a PyTorch dataset class for loading `.npy` image files, converting them to 3-channel tensors, and applying transformations. The `create_dataloaders` function splits the dataset into training and validation sets and creates corresponding DataLoaders.\n","metadata":{}},{"cell_type":"code","source":"class NpyDataset(Dataset):\n    def __init__(self, folder_path, transform=None):\n        self.folder_path = folder_path\n        self.transform = transform\n        self.data = []\n        self.labels = []\n        class_folders = sorted(os.listdir(folder_path))  # Ensure class order consistency\n        \n        for class_idx, class_folder in enumerate(class_folders):\n            class_path = os.path.join(folder_path, class_folder)\n            if not os.path.isdir(class_path):\n                continue\n            \n            for file_name in os.listdir(class_path):\n                if file_name.endswith(\".npy\"):\n                    file_path = os.path.join(class_path, file_name)\n                    self.data.append(file_path)\n                    self.labels.append(class_idx)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        file_path = self.data[idx]\n        array = np.load(file_path, allow_pickle=True)  # Load .npy file\n        first_obj = array[0]  # Take only the first object (64, 64)\n        first_obj = torch.tensor(first_obj, dtype=torch.float32).unsqueeze(0)  # Convert to tensor and add channel dim\n        first_obj = first_obj.repeat(3, 1, 1)  # Convert grayscale to 3-channel format\n        \n        if self.transform:\n            first_obj = self.transform(first_obj)\n        \n        label = self.labels[idx]\n        return first_obj, label\n\ndef create_dataloaders(folder_path, batch_size=32, shuffle=True, val_split=0.1):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  # Resize to (224, 224)\n    ])\n    \n    dataset = NpyDataset(folder_path, transform=transform)\n    \n    val_size = int(len(dataset) * val_split)\n    train_size = len(dataset) - val_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:38.466927Z","iopub.execute_input":"2025-03-27T05:42:38.467290Z","iopub.status.idle":"2025-03-27T05:42:38.476696Z","shell.execute_reply.started":"2025-03-27T05:42:38.467263Z","shell.execute_reply":"2025-03-27T05:42:38.475755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation Metrics: Accuracy and ROC-AUC  \nDefines an `accuracy` function to compute classification accuracy and a `compute_roc_auc` function to calculate the ROC curve and AUC scores for a multi-class classification problem using a one-vs-rest approach.\n","metadata":{}},{"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs.logits, dim = 1)\n    return torch.sum(preds == labels).item() / len(labels)\n\n# Function to compute ROC curve and AUC for each class\ndef compute_roc_auc(all_labels, all_logits):\n    # convert to numpy array \n    all_labels = all_labels.cpu().numpy()\n    all_logits = all_logits.cpu().numpy()\n\n    n_classes = 3\n\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n\n    # for each class compute the ROC curve using one-vs-rest approach\n    for i in range (n_classes):\n        fpr[i], tpr[i], _ = roc_curve(all_labels == i, all_logits[:,i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(\n        np.eye(n_classes)[all_labels].ravel(), all_logits.ravel()\n    )\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    return fpr, tpr, roc_auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:42.669795Z","iopub.execute_input":"2025-03-27T05:26:42.670146Z","iopub.status.idle":"2025-03-27T05:26:42.675795Z","shell.execute_reply.started":"2025-03-27T05:26:42.670121Z","shell.execute_reply":"2025-03-27T05:26:42.674874Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Patch Embedding Layer for Vision Transformer  \nImplements `PatchEmbed`, a PyTorch module that converts an input image into a sequence of patch embeddings using a convolutional layer. This is a key component of Vision Transformers (ViTs), where images are divided into patches and projected into an embedding space.\n","metadata":{}},{"cell_type":"code","source":"class PatchEmbed(nn.Module):\n    def __init__(self,\n                img_size = 224,\n                patch_size = 16,\n                in_chans = 3,\n                embed_dim = 768):\n        super().__init__()\n        self.img_size = (img_size, img_size)\n        self.patch_size = (patch_size, patch_size)\n        self.num_patches = (img_size//patch_size)**2\n\n        self.proj = nn.Conv2d(in_chans,\n                              embed_dim,\n                              kernel_size = patch_size,\n                              stride = patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x)\n        x = x.flatten(2)\n        x = x.transpose(1, 2)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:42.808435Z","iopub.execute_input":"2025-03-27T05:26:42.808685Z","iopub.status.idle":"2025-03-27T05:26:42.813781Z","shell.execute_reply.started":"2025-03-27T05:26:42.808666Z","shell.execute_reply":"2025-03-27T05:26:42.812852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Transformer Encoder for Vision Transformer  \nDefines `TransformerEncoder`, a stack of transformer blocks used in Vision Transformers (ViTs). It consists of multiple attention layers with normalization and dropout, processing image patch embeddings sequentially.\n","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self,\n                embed_dim = 768,\n                depth = 12,\n                num_heads = 12,\n                mlp_ratio = 4,\n                qkv_bias = True,\n                drop_rate = 0.,\n                attn_drop_rate = 0.,\n                drop_path_rate = 0.):\n        super().__init__()\n        # Create a sequence of transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim = embed_dim,\n                num_heads = num_heads,\n                mlp_ratio = mlp_ratio,\n                qkv_bias = qkv_bias,\n                drop = drop_rate,\n                attn_drop = attn_drop_rate,\n                # Stocastic depth: gradually increase drop_path rate for deeper blocks\n                drop_path = drop_path_rate*i/depth)\n            for i in range(depth)])\n        self.norm = nn.LayerNorm(embed_dim) # Final norm Layer\n\n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        x = self.norm(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:48.995143Z","iopub.execute_input":"2025-03-27T05:26:48.995447Z","iopub.status.idle":"2025-03-27T05:26:49.001311Z","shell.execute_reply.started":"2025-03-27T05:26:48.995425Z","shell.execute_reply":"2025-03-27T05:26:49.000327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Transformer Block for Vision Transformer  \nDefines `TransformerBlock`, a core component of the Vision Transformer (ViT). Each block consists of layer normalization, multi-head self-attention, and a feed-forward MLP with residual connections for efficient feature learning.\n","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self,\n                dim,\n                num_heads,\n                mlp_ratio = 4.,\n                qkv_bias = False,\n                drop = 0.,\n                attn_drop = 0.,\n                drop_path = 0.):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim) #First Layer Norm\n        self.attn = Attention(dim,\n                              num_heads = num_heads,\n                             qkv_bias = qkv_bias,\n                             attn_drop = attn_drop,\n                             proj_drop = drop) #Multi-Head attention\n        \n        self.norm2 = nn.LayerNorm(dim) #Second Layer Norm\n        mlp_hidden_dim = int(dim * mlp_ratio) # Hidden dimentions of MLP\n        self.mlp = Mlp(in_features = dim,\n                      hidden_features = mlp_hidden_dim,\n                      drop = drop)\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x)) #Self-attention block with residual connection\n        x = x + self.mlp(self.norm2(x)) #Mlp Block with residual connection\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:49.548526Z","iopub.execute_input":"2025-03-27T05:26:49.548880Z","iopub.status.idle":"2025-03-27T05:26:49.554545Z","shell.execute_reply.started":"2025-03-27T05:26:49.548850Z","shell.execute_reply":"2025-03-27T05:26:49.553586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Multi-Head Self-Attention Mechanism  \nImplements `Attention`, a multi-head self-attention module used in Vision Transformers (ViTs). It computes relationships between tokens using scaled dot-product attention, applies dropout, and projects the output back to the embedding dimension.\n","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self,\n                dim,\n                num_heads = 8,\n                qkv_bias = False,\n                attn_drop = 0.,\n                proj_drop = 0.):\n        super().__init__()\n        self.num_heads = num_heads # Number of attention Heads\n        head_dim = dim//num_heads #Dimension of each Head\n        self.scale = head_dim ** -0.5 #Scaling Factor for Dot product\n        \n        #Linear Proj for Q,K,V for all heads simultaneously\n        self.qkv = nn.Linear(dim, dim*3, bias = qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop) # Dropout for attention Matrix\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)  # Dropout for output Projection\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C//self.num_heads).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2] #Shape: [B, H, N, C/H]\n\n        # Compute scaled dot-product attention\n        # (q@k.transpose) calculates similarity between query and key vectors\n        attn = (q@k.transpose(-2, -1))*self.scale #B, H, N, N\n        attn = attn.softmax(dim = -1)\n        attn = self.attn_drop(attn)\n\n        # Apply attention weights to values\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        #Project back to embeding dimention\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:49.771653Z","iopub.execute_input":"2025-03-27T05:26:49.771998Z","iopub.status.idle":"2025-03-27T05:26:49.778665Z","shell.execute_reply.started":"2025-03-27T05:26:49.771969Z","shell.execute_reply":"2025-03-27T05:26:49.777890Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Multi-Layer Perceptron (MLP) for Vision Transformer  \nDefines `Mlp`, a feed-forward neural network with two linear layers, GELU activation, and dropout. It is used in transformer blocks to process token embeddings after the attention mechanism.\n","metadata":{}},{"cell_type":"code","source":"class Mlp(nn.Module):\n    \"\"\"\n    Multi-layer preceptron: Implements a simple feed forward network with one hidden layer and GELU activation\n    \"\"\"\n    def __init__(self,\n                in_features,\n                hidden_features = None,\n                out_features = None,\n                drop = 0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        # Two-Layer MLP\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        \"\"\"\n        Input: B × N × in_features\n        Output: B × N × out_features\n        \"\"\"\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:49.942056Z","iopub.execute_input":"2025-03-27T05:26:49.942369Z","iopub.status.idle":"2025-03-27T05:26:49.947665Z","shell.execute_reply.started":"2025-03-27T05:26:49.942347Z","shell.execute_reply":"2025-03-27T05:26:49.946902Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Transformer Decoder for Vision Transformer  \nImplements `TransformerDecoder`, a stack of transformer blocks designed to process encoded representations. It consists of multiple self-attention layers, feed-forward MLPs, and layer normalization to refine feature embeddings.\n","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n    def __init__(self,\n                embed_dim = 768,\n                depth = 8,\n                num_heads = 16,\n                mlp_ratio = 4.,\n                qkv_bias = True,\n                drop_rate = 0.,\n                attn_drop_rate = 0.,\n                drop_path_rate = 0.):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim = embed_dim,\n                num_heads = num_heads,\n                mlp_ratio = mlp_ratio,\n                qkv_bias = qkv_bias,\n                drop = drop_rate,\n                attn_drop = attn_drop_rate,\n                drop_path = drop_path_rate * i / depth\n            ) for i in range(depth)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n        \n    def forward(self, x):\n        for block in self.blocks:\n            x  = block(x)\n        x = self.norm(x)\n        return x\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:50.090262Z","iopub.execute_input":"2025-03-27T05:26:50.090566Z","iopub.status.idle":"2025-03-27T05:26:50.096252Z","shell.execute_reply.started":"2025-03-27T05:26:50.090544Z","shell.execute_reply":"2025-03-27T05:26:50.095380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Masked Autoencoder (MAE) Implementation\nThis class implements a Masked Autoencoder (MAE) using a Vision Transformer (ViT) backbone. It randomly masks image patches, encodes visible patches with a transformer encoder, and reconstructs the full image using a transformer decoder. The model is trained using an MSE loss calculated only on masked patches.\n","metadata":{}},{"cell_type":"code","source":"class MaskedAutoEncoder(nn.Module):\n    def __init__(self,\n                img_size = 224,\n                patch_size = 16,\n                in_chans = 3,\n                embed_dim = 1024,\n                depth = 24,\n                num_heads = 16,\n                decoder_embed_dim = 512,\n                decoder_depth = 8,\n                decoder_num_heads = 16,\n                mlp_ratio = 4.,\n                norm_layer = nn.LayerNorm):\n        super().__init__()\n        # Encoder Components\n        # PatchEmbed splits the image into patches and embeds them\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n\n        # Class token and Positional Encoding for encoder\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n\n        # Encoder for the visible Patches\n        self.encoder = TransformerEncoder(\n            embed_dim = embed_dim,\n            depth = depth,\n            num_heads = num_heads,\n            mlp_ratio = mlp_ratio\n        )\n\n        # Decoder components\n        # Convert the encoder output to decoder dimension\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias = True)\n        # Learnable mask token that is used for masked patches\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n        # Positional encoding ffor the decoder\n        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim))\n        \n        # Decoder to reconstruct the full image\n        self.decoder = TransformerDecoder(\n            embed_dim = decoder_embed_dim,\n            depth = decoder_depth,\n            num_heads = decoder_num_heads,\n            mlp_ratio = mlp_ratio\n            )\n        # Final prediction Layer: predict pixel values for eeach patch\n        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias = True)\n        \n        # Initialize weights for all components\n        self.initialize_weights()\n\n        # Store model parameters for later use\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n        self.img_size = img_size\n\n    def initialize_weights(self):\n        pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.pos_embed.shape[2]))\n        decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.decoder_pos_embed.shape[2]))\n        \n        \n        # Use truncated normal distributions \n        nn.init.trunc_normal_(self.pos_embed, std = 0.02)\n        nn.init.trunc_normal_(self.decoder_pos_embed, std = 0.02)\n\n        nn.init.trunc_normal_(self.cls_token, std = 0.02)\n        nn.init.trunc_normal_(self.mask_token, std = 0.02)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std = 0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def random_masking(self, x, mask_ratio):\n        N, L, D = x.shape # Batch, length, dimension\n        len_keep = int(L * (1 - mask_ratio)) # Number of patches to keep\n\n        # Generate uniform random noise for each patch in each sample\n        noise = torch.rand(N, L, device = x.device) \n\n        # Sort noise to determine which patches to keep/remove\n        ids_shuffle = torch.argsort(noise, dim=1)\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n        # Keep the first len_keep patches (lowest noise values)\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, dim=1, index = ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n        mask = torch.ones([N, L], device = x.device)\n        mask[:, :len_keep] = 0\n        # Unshuffle to get the binary mask for original sequence\n        mask = torch.gather(mask, dim = 1, index = ids_restore)\n\n        return x_masked, mask, ids_restore\n\n    def forward_encoder(self, x, mask_ratio):\n        # Convert img to patches\n        x = self.patch_embed(x)\n    \n        # Add positional embeddings\n        cls_token = self.cls_token + self.pos_embed[:, :1, :]  # [1, 1, D]\n        x = x + self.pos_embed[:, 1:, :]  # [B, N, D]\n    \n        # Apply random masking\n        x, mask, ids_restore = self.random_masking(x, mask_ratio)  # [B, N', D]\n\n        # Expand class token to match batch size\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)  # [B, 1, D]\n\n        # Concatenate cls_token and image tokens\n        x = torch.cat((cls_tokens, x), dim=1)  # [B, N'+1, D]\n\n        # Process through transformer encoder\n        x = self.encoder(x)\n\n        return x, mask, ids_restore\n\n\n    def forward_decoder(self, x, ids_restore):\n        # embed the encoder output\n        x = self.decoder_embed(x)\n\n        # add mask tokens\n        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n\n        # exclude class token x[:, 1:] and append mask tokens\n        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim = 1)\n\n        # unshuffle: restore the original sequence order\n        x_ = torch.gather(x_, dim = 1, index = ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))\n\n        # append class token\n        x = torch.cat([x[:, :1, :], x_], dim = 1)\n\n        # apply positional embedding \n        x = x + self.decoder_pos_embed\n\n        # apply transformer decoder \n        x = self.decoder(x)\n\n        # predict pixel values for each patch\n        x = self.decoder_pred(x)\n\n        # remove class token from prediictions\n        x = x[:, 1:, :]\n\n        return x\n\n    def forward(self, imgs, mask_ratio = 0.75):\n        # Forward Pass the entire MAE model\n\n        # run encoder on the images with masking\n        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n\n        # run decoder to predict all patches\n        pred = self.forward_decoder(latent, ids_restore)\n\n        # convert input images to patches for loss calculation\n        target = self.patchify(imgs)\n\n        # calculate mse loss only for masked patches\n        loss = self.calculate_loss(pred, target, mask)\n\n        return loss, pred, mask\n\n    def patchify(self, imgs):\n        # convert imgs to patches for calculating loss\n        p = self.patch_size\n        h = w = self.img_size//p\n\n        x = rearrange(imgs, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n        return x\n\n    def unpatchify(self, x):\n        p = self.patch_size\n        h = w = int(x.shape[1] ** 0.5)\n\n        imgs = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h = h, w = w, p1 = p, p2 = p)\n        return imgs\n\n    def calculate_loss(self, pred, target, mask):\n        # calculate mse loss for masked patches only\n\n        loss = (pred - target)**2\n        loss = loss.mean(dim = -1)\n\n        loss = (loss*mask).sum()/mask.sum() \n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:50.238265Z","iopub.execute_input":"2025-03-27T05:26:50.238570Z","iopub.status.idle":"2025-03-27T05:26:50.256759Z","shell.execute_reply.started":"2025-03-27T05:26:50.238548Z","shell.execute_reply":"2025-03-27T05:26:50.255895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MAEClassifier(nn.Module):\n    def __init__(self, mae_model, num_classes):\n        super().__init__()\n        \n        # Use the pre-trained MAE encoder\n        self.patch_embed = mae_model.patch_embed\n        self.cls_token = mae_model.cls_token\n        self.pos_embed = mae_model.pos_embed\n        self.encoder = mae_model.encoder\n        \n        # Determine the embedding dimension dynamically\n        embed_dim = mae_model.encoder.blocks[0].norm1.weight.shape[0]\n        \n        # Freeze encoder weights\n        for param in self.patch_embed.parameters():\n            param.requires_grad = False\n        for param in self.encoder.parameters():\n            param.requires_grad = True\n        \n        # Classification head\n        self.classification_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n    \n    def forward(self, x):\n        # Patch embedding\n        x = self.patch_embed(x)\n    \n        # Add positional embeddings\n        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n        x = x + self.pos_embed[:, 1:, :]\n        \n        # Expand class token to match batch size\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        \n        # Concatenate cls_token and image tokens\n        x = torch.cat((cls_tokens, x), dim=1)\n        \n        # Encode\n        x = self.encoder(x)\n        \n        # Use cls token for classification\n        x = x[:, 0]\n        \n        # Classification\n        return self.classification_head(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:36:51.177849Z","iopub.execute_input":"2025-03-27T05:36:51.178240Z","iopub.status.idle":"2025-03-27T05:36:51.186267Z","shell.execute_reply.started":"2025-03-27T05:36:51.178211Z","shell.execute_reply":"2025-03-27T05:36:51.185251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training Function for MAE Classifier\nThis function trains a Masked Autoencoder (MAE) classifier using cross-entropy loss and AdamW optimizer. It employs mixed precision training, cosine annealing learning rate scheduling, and calculates multi-class AUC-ROC scores for performance evaluation on both training and validation sets.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\ndef train_classifier(model, train_loader, val_loader, num_epochs=5, learning_rate=5e-4):\n    # Loss and Optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(\n        model.parameters(),  \n        lr=learning_rate, \n        weight_decay=0.05\n    )\n    \n    # Learning Rate Scheduler\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, \n        T_max=num_epochs\n    )\n    \n    # Mixed Precision Training\n    scaler = GradScaler()\n    \n    # Device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    # Training Loop\n    for epoch in range(num_epochs):\n        # Training Phase\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        # Containers for AUC-ROC calculation\n        train_true_labels = []\n        train_pred_probs = []\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Mixed precision forward pass\n            # with autocast():\n            #     outputs = model(images)\n            #     loss = criterion(outputs, labels)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            # Backward pass with gradient scaling\n            # scaler.scale(loss).backward()\n            # scaler.step(optimizer)\n            # scaler.update()\n            \n            # Metrics\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            train_total += labels.size(0)\n            train_correct += predicted.eq(labels).sum().item()\n            \n            # Collect data for AUC-ROC\n            train_true_labels.extend(labels.cpu().numpy())\n            train_pred_probs.extend(torch.softmax(outputs, dim=1).cpu().detach().numpy())\n        \n        # Validation Phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        # Containers for AUC-ROC calculation\n        val_true_labels = []\n        val_pred_probs = []\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                \n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n                \n                # Collect data for AUC-ROC\n                val_true_labels.extend(labels.cpu().numpy())\n                val_pred_probs.extend(torch.softmax(outputs, dim=1).cpu().detach().numpy())\n        \n        # Calculate per-class AUC-ROC for Training Set\n        train_auc_roc = []\n        for cls in range(3):  # Assuming 3 classes\n            train_binary_labels = np.array([(label == cls).astype(int) for label in train_true_labels])\n            train_class_probs = np.array([pred_prob[cls] for pred_prob in train_pred_probs])\n            train_auc = roc_auc_score(train_binary_labels, train_class_probs)\n            train_auc_roc.append(train_auc)\n        \n        # Calculate per-class AUC-ROC for Validation Set\n        val_auc_roc = []\n        for cls in range(3):  # Assuming 3 classes\n            val_binary_labels = np.array([(label == cls).astype(int) for label in val_true_labels])\n            val_class_probs = np.array([pred_prob[cls] for pred_prob in val_pred_probs])\n            val_auc = roc_auc_score(val_binary_labels, val_class_probs)\n            val_auc_roc.append(val_auc)\n        \n        # Learning rate step\n        scheduler.step()\n        \n        # Print metrics\n        print(f'Epoch [{epoch+1}/{num_epochs}]')\n        print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n              f'Train Accuracy: {100*train_correct/train_total:.2f}%')\n        print(f'Train AUC-ROC per class: {[f\"{auc:.4f}\" for auc in train_auc_roc]}')\n        \n        print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n              f'Val Accuracy: {100*val_correct/val_total:.2f}%')\n        print(f'Val AUC-ROC per class: {[f\"{auc:.4f}\" for auc in val_auc_roc]}')\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:26:59.617312Z","iopub.execute_input":"2025-03-27T05:26:59.617603Z","iopub.status.idle":"2025-03-27T05:26:59.625970Z","shell.execute_reply.started":"2025-03-27T05:26:59.617583Z","shell.execute_reply":"2025-03-27T05:26:59.625040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_mae_weights(model, checkpoint_path):\n    # Load the checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        \n    # Load the state dict directly\n    model.load_state_dict(checkpoint)\n    print(\"Successfully loaded weights.\")\n        \n    return checkpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:27:03.188340Z","iopub.execute_input":"2025-03-27T05:27:03.188658Z","iopub.status.idle":"2025-03-27T05:27:03.194333Z","shell.execute_reply.started":"2025-03-27T05:27:03.188633Z","shell.execute_reply":"2025-03-27T05:27:03.193615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = \"/kaggle/input/dataset-task-6/Dataset\"\ntrain_loader, val_loader = create_dataloaders(path, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:47.144176Z","iopub.execute_input":"2025-03-27T05:42:47.144480Z","iopub.status.idle":"2025-03-27T05:42:47.286520Z","shell.execute_reply.started":"2025-03-27T05:42:47.144459Z","shell.execute_reply":"2025-03-27T05:42:47.285801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Usage\ncheckpoint_path = '/kaggle/input/pretrained-masked-auto-encoder/pytorch/default/1/model.pth'\nmae_model = MaskedAutoEncoder()\ncheckpoint = load_mae_weights(mae_model, checkpoint_path)\n\n# Create the classifier\nnum_classes = 3\nmae_classifier = MAEClassifier(mae_model, num_classes)\n\n# Fine-tune the classifier\ntrained_model = train_classifier(mae_classifier, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:52.471276Z","iopub.execute_input":"2025-03-27T05:42:52.471628Z","iopub.status.idle":"2025-03-27T05:43:26.625014Z","shell.execute_reply.started":"2025-03-27T05:42:52.471598Z","shell.execute_reply":"2025-03-27T05:43:26.623841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"/kaggle/working/finetuned_mae.pth\"\ntorch.save(trained_model.state_dict(), model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}