{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10906759,"sourceType":"datasetVersion","datasetId":6779285}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shobhiii/common-test-1-ml4sci?scriptVersionId=230354618\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Vision Transformer (ViT) Setup for Multiclass Image Classification  \n\nThis script imports essential libraries for loading datasets, preprocessing images, training a **ViT-based classifier**, and evaluating its performance using PyTorch and `transformers`.  \n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor\nfrom torchvision.transforms import functional as F\nfrom torchvision import transforms\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, precision_score, recall_score, f1_score\nfrom itertools import cycle","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_roc_curves(fpr, tpr, roc_auc, class_names):\n    plt.figure(figsize=(10, 8))\n    \n    # Plot ROC curve for each class\n    colors = cycle(['blue', 'red', 'green'])\n    for i, color in zip(range(len(class_names)), colors):\n        plt.plot(\n            fpr[i], \n            tpr[i], \n            color=color, \n            lw=2,\n            label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})'\n        )\n    \n    # Plot micro-average ROC curve\n    plt.plot(\n        fpr[\"micro\"], \n        tpr[\"micro\"],\n        color='deeppink', \n        linestyle=':', \n        linewidth=4,\n        label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})'\n    )\n    \n    # Plot diagonal line (random classifier)\n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curves')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    plt.savefig('roc_curves.png', dpi=300)\n    plt.close()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Loading and Transformation  \n\nDefines functions to load `.npy` image files, apply augmentations for training, and normalize images for evaluation. Creates PyTorch dataloaders for training, validation, and testing.  \n \n","metadata":{}},{"cell_type":"code","source":"def npy_loader(path):\n    tensor = torch.from_numpy(np.load(path))\n\n    # the tensor has shape [1,150,150] - single channel. so we will repeat the single channel to create 3 identical channels\n    if tensor.dim() == 3 and tensor.shape[0] == 1:\n        tensor = tensor.repeat(3, 1, 1)\n    return tensor","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transform pipeline for training data\ntrain_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomResizedCrop(224, scale = (0.8, 1.0)), #random crops with some zoom\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(0.1),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndef train_loader(path):\n    tensor = npy_loader(path)\n    return train_transforms(tensor)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transform pipeline for validation and test data\neval_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndef eval_loader(path):\n    tensor = npy_loader(path)\n    return eval_transforms(tensor)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data_path = \"/kaggle/input/common-test-1-dataset/dataset/train\"\ntest_data_path = \"/kaggle/input/common-test-1-dataset/dataset/val\"\n\n# Load the full training dataset\nfull_train_dataset = datasets.DatasetFolder(\n    root=train_data_path,\n    loader=train_loader,\n    extensions=('.npy',)  \n)\n\n# Load the test dataset \ntest_dataset = datasets.DatasetFolder(\n    root=test_data_path,\n    loader=eval_loader,\n    extensions=('.npy',)\n)\n\ntrain_size = int(0.9 * len(full_train_dataset))\nval_size = len(full_train_dataset) - train_size\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    full_train_dataset, \n    [train_size, val_size]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T16:15:49.06619Z","iopub.execute_input":"2025-03-24T16:15:49.06648Z","iopub.status.idle":"2025-03-24T16:15:49.139523Z","shell.execute_reply.started":"2025-03-24T16:15:49.066448Z","shell.execute_reply":"2025-03-24T16:15:49.138254Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create data loaders\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=32, \n    shuffle=True, \n    num_workers=4, \n    pin_memory=True\n)\n\nval_dataloader = DataLoader(\n    val_dataset,  \n    batch_size=32, \n    shuffle=False,\n    num_workers=4, \n    pin_memory=True\n)\n\ntest_dataloader = DataLoader(\n    test_dataset, \n    batch_size=32, \n    shuffle=False, \n    num_workers=4, \n    pin_memory=True\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Initialization  \n\nLoads a pre-trained Vision Transformer (ViT) model and feature extractor, adapting it for a 3-class classification task.  \n","metadata":{}},{"cell_type":"code","source":"model_name = \"google/vit-base-patch16-224\"\nfeature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\nmodel = ViTForImageClassification.from_pretrained(model_name,\n                                                 num_labels = 3,\n                                                 ignore_mismatched_sizes=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Class-to-label mapping:\", train_dataset.dataset.class_to_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Fine-Tuning Setup  \n\nDefines the loss function (**CrossEntropyLoss**), optimizer (**AdamW** with weight decay), and learning rate scheduler (**CosineAnnealingLR**). Moves the model to GPU if available.  \n","metadata":{}},{"cell_type":"code","source":"#  FINE TUNING(LOSS FUNCTION, OPTIMIZER AND SCHEDULER)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5, weight_decay = 0.05)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation Metrics  \n\nDefines functions to compute **accuracy** and **ROC-AUC** for a 3-class classification task. The `compute_roc_auc` function calculates **one-vs-rest ROC curves** and micro-average AUC.  \n","metadata":{}},{"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs.logits, dim = 1)\n    return torch.sum(preds == labels).item() / len(labels)\n\n# Function to compute ROC curve and AUC for each class\ndef compute_roc_auc(all_labels, all_logits):\n    # convert to numpy array \n    all_labels = all_labels.cpu().numpy()\n    all_logits = all_logits.cpu().numpy()\n\n    n_classes = 3\n\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n\n    # for each class compute the ROC curve using one-vs-rest approach\n    for i in range (n_classes):\n        fpr[i], tpr[i], _ = roc_curve(all_labels == i, all_logits[:,i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(\n        np.eye(n_classes)[all_labels].ravel(), all_logits.ravel()\n    )\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    return fpr, tpr, roc_auc","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 15\nbest_val_acc = 0.0\nbest_val_metric = 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training and Validation Loop  \n\nImplements the **training and evaluation process** for fine-tuning ViT:  \n- Performs forward and backward passes, updates weights, and tracks loss & accuracy.  \n- Computes **ROC-AUC** for multi-class classification.  \n- Saves the best model based on **micro-average AUC**.  \n- Optionally plots ROC curves at the last epoch.  \n","metadata":{}},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train\n    train_loss = 0.0\n    train_acc = 0.0\n\n    train_bar = tqdm(train_dataloader, desc = f\"Epoch {epoch + 1}/{num_epochs}[train]\")\n    for images, labels in train_bar:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(pixel_values = images, labels = labels)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n\n        # TRACK STATISTICS\n        train_loss += loss.item()*images.size(0)\n        train_acc += accuracy(outputs, labels)*images.size(0)\n\n        # Update progress bar\n        train_bar.set_postfix({\n            \"loss\":loss.item(),\n            \"LR\":optimizer.param_groups[0][\"lr\"]\n        })\n\n    # Calculate average training statistics\n    train_loss = train_loss/len(train_dataset)\n    train_acc = train_acc/len(train_dataset)\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    val_acc = 0.0\n    all_labels = []\n    all_logits = []\n    \n    with torch.no_grad():\n        val_bar = tqdm(val_dataloader, desc = f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n        for images, labels in val_bar:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(pixel_values=images, labels = labels)\n            loss = outputs.loss\n\n            # Track Statistics\n            val_loss += loss.item()*images.size(0)\n            val_acc += accuracy(outputs, labels)*images.size(0)\n\n            # Collect predictions and labels for ROC-AUC calculation\n            all_labels.append(labels)\n            all_logits.append(outputs.logits)\n            \n            # Update progress bar\n            val_bar.set_postfix({\n                \"Loss\":loss.item()\n            })\n\n    # Concatenate all batches\n    all_labels = torch.cat(all_labels)\n    all_logits = torch.cat(all_logits)\n\n    # Compute ROC-AUC\n    fpr, tpr, roc_auc = compute_roc_auc(all_labels, all_logits)\n    \n    # Calculate average validation statistics\n    val_loss = val_loss/len(val_dataset)\n    val_acc = val_acc/len(val_dataset)\n\n    # Update learning rate\n    scheduler.step()\n\n    # Print epoch summary\n    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n         f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n         f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n    # Print AUC for each class\n    for i in range(3):\n        class_name = train_dataset.dataset.classes[i]\n        print(f\"AUC for Class {class_name}: {roc_auc[i]:.4f}\")\n\n    # print micro-average AUC\n    print(f\"Micro-average AUC: {roc_auc['micro']:.4f}\")\n    \n    # Save best model based on micro-average AUC instead of accuracy\n    if roc_auc['micro'] > best_val_metric:\n        best_val_metric = roc_auc['micro']\n        model.save_pretrained(\"./best_vit_model\")\n        print(f\"Model saved with best micro-average AUC: {best_val_metric:.4f}\")\n\n    # Optionally, plot and save ROC curves\n    if epoch == num_epochs - 1:  # Plot on the last epoch\n        plot_roc_curves(fpr, tpr, roc_auc, train_dataset.dataset.classes)   \nprint(f\"Training complete! Best micro-average AUC: {best_val_metric:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the best saved model\nmodel_path = \"./best_vit_model\"\nmodel = ViTForImageClassification.from_pretrained(model_path)\nmodel.to(device)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Evaluation on Test Set  \n\nDefines a function to evaluate the model on the test dataset by computing **loss, accuracy, and predictions**. Tracks performance metrics and updates a progress bar.  \n","metadata":{}},{"cell_type":"code","source":"# Evaluation on test set\ndef evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    test_loss = 0.0\n    test_acc = 0.0\n    all_labels = []\n    all_logits = []\n    all_preds = []\n    \n    with torch.no_grad():\n        test_bar = tqdm(dataloader, desc=\"Testing\")\n        for images, labels in test_bar:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(pixel_values=images, labels=labels)\n            loss = outputs.loss\n            \n            # Get predictions\n            _, preds = torch.max(outputs.logits, dim=1)\n            \n            # Track statistics\n            test_loss += loss.item() * images.size(0)\n            test_acc += torch.sum(preds == labels).item()\n            \n            # Collect for metrics calculation\n            all_labels.append(labels)\n            all_logits.append(outputs.logits)\n            all_preds.append(preds)\n            \n            # Update progress bar\n            test_bar.set_postfix({\n                \"Loss\": loss.item()\n            })\n    \n    # Calculate final metrics\n    test_loss = test_loss / len(dataloader.dataset)\n    test_acc = test_acc / len(dataloader.dataset)\n    \n    # Concatenate all batches\n    all_labels = torch.cat(all_labels)\n    all_logits = torch.cat(all_logits)\n    all_preds = torch.cat(all_preds)\n    \n    return test_loss, test_acc, all_labels, all_logits, all_preds\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run evaluation\ntest_loss, test_acc, all_labels, all_logits, all_preds = evaluate_model(\n    model, test_dataloader, criterion, device\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test Set Performance Metrics  \n\nComputes and prints key evaluation metrics:  \n- **ROC-AUC** for each class and micro-average.  \n- **Confusion matrix** for classification results.  \n- **Precision, Recall, and F1-score** for each class, including weighted F1-score.  \n","metadata":{}},{"cell_type":"code","source":"# Compute ROC-AUC\nfpr, tpr, roc_auc = compute_roc_auc(all_labels, all_logits)\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(all_labels.cpu().numpy(), all_preds.cpu().numpy())\n\n# Calculate precision, recall, and F1 score\nprecision = precision_score(all_labels.cpu().numpy(), all_preds.cpu().numpy(), average=None)\nrecall = recall_score(all_labels.cpu().numpy(), all_preds.cpu().numpy(), average=None)\nf1 = f1_score(all_labels.cpu().numpy(), all_preds.cpu().numpy(), average=None)\nweighted_f1 = f1_score(all_labels.cpu().numpy(), all_preds.cpu().numpy(), average='weighted')\n\n# Print results\nprint(\"\\n--- Test Set Evaluation Results ---\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n\n# Print AUC for each class\nprint(\"\\n--- AUC Scores per Class ---\")\nfor i in range(3):\n    class_name = test_dataset.classes[i]\n    print(f\"Class {class_name}: {roc_auc[i]:.4f}\")\nprint(f\"Micro-average AUC: {roc_auc['micro']:.4f}\")\n\n# Print precision, recall, and F1 score for each class\nprint(\"\\n--- Precision, Recall, and F1 Score per Class ---\")\nfor i in range(3):\n    class_name = test_dataset.classes[i]\n    print(f\"Class {class_name}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display confusion matrix\nprint(\"\\n--- Confusion Matrix ---\")\nclass_names = test_dataset.classes\ndf_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\nprint(df_cm)\n\n# Plot ROC curves\nplt.figure(figsize=(10, 8))\nfor i in range(3):\n    class_name = test_dataset.classes[i]\n    plt.plot(fpr[i], tpr[i], \n             label=f'Class {class_name} (AUC = {roc_auc[i]:.4f})')\n\nplt.plot(fpr['micro'], tpr['micro'],\n         label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.4f})',\n         linestyle=':', linewidth=4)\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves for Test Set')\nplt.legend(loc=\"lower right\")\nplt.savefig('test_roc_curves.png')\nplt.close()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save the model to Kaggle output directory\n# output_path = \"/kaggle/working/vit_model_final\"\n# model.save_pretrained(output_path)\n# print(f\"\\nModel saved to {output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}